{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skiing\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook an agent of your creation will learn [how to ski](https://gym.openai.com/envs/Skiing-ram-v0/).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\") # root of all code is 2 floors below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import QNetwork\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space:  Box(128,)\n",
      "observation_space.high: \n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255]\n",
      "observation_space.low: \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Action space:  Discrete(3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dacostlu/miniconda3/envs/gym/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Skiing-ram-v0')\n",
    "print(\"observation_space: \", env.observation_space)\n",
    "print(\"observation_space.high: \\n\", env.observation_space.high)\n",
    "print(\"observation_space.low: \\n\", env.observation_space.low)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 79  13 247  15 247  12 247  14 247  10 247   0   0   0 128   8   7   0\n",
      " 128  64 128 234 246 126 247  76 120   0   1 134  28 234  80   6  52  98\n",
      " 160 222 128   0   2 133 133   4   2 131 126 157 111 126 126 157 111 126\n",
      " 255 255 255 255  31   7  15  31  79   7  30 155 152  59  41 128  85   5\n",
      "   2  85  85   5   2  85   6   5   0   3   2   1   0   7 195 176 214 160\n",
      "  80  40  24   4   4   0   0   0   0   0   0   0   0   0   0   0   0  32\n",
      " 170 255   0 255 167 215  85 137   0   0  32   1  63   0 240  72  14 244\n",
      "  44 241]\n",
      "241\n",
      "128\n",
      "<class 'numpy.ndarray'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(observation)\n",
    "print(observation[127])\n",
    "state_size = len(observation)\n",
    "print(state_size)\n",
    "print(type(observation))\n",
    "action_size = env.action_space.n\n",
    "print(action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "episode = 1\n",
    "reward_on_episode = 0\n",
    "rewards_per_episode = []\n",
    "for i in range(10000):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    reward_on_episode += reward\n",
    "    if done:\n",
    "        print(\"DONE episode %d; reward obtained = %.2f\" % (episode, reward_on_episode))\n",
    "        episode += 1\n",
    "        rewards_per_episode.append(reward_on_episode)\n",
    "        reward_on_episode = 0\n",
    "        env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(rewards_per_episode)), rewards_per_episode)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Skiing randomly')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an agent\n",
    "from dqn_agent import DQNAgent, DoubleDQNAgent\n",
    "from memory import ReplayBuffer, WeightedReplayBuffer\n",
    "\n",
    "hidden_sizes = [\n",
    "    int(round(state_size * .8)), \n",
    "    int(round(state_size * .6)), \n",
    "    int(round(state_size * .4)), \n",
    "    int(round(action_size * 2))]\n",
    "# self.hidden_sizes = [int(round(state_size * 10)), int(round(state_size * 5)), int(round(action_size * 10))]\n",
    "fc = nn.Sequential(\n",
    "    nn.Linear(state_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[3], action_size)\n",
    ")\n",
    "fc2 = nn.Sequential(\n",
    "    nn.Linear(state_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[3], action_size)\n",
    ")\n",
    "\n",
    "main_model = QNetwork(name=\"main\", fc=fc)\n",
    "target_model = QNetwork(name=\"target\", fc=fc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dqn = DQNAgent(\n",
    "    main_model=main_model,\n",
    "    target_network=target_model,\n",
    "    update_every_steps=10, \n",
    "    lr=1e-3)\n",
    "\n",
    "agent_dqn_prioritized_replay = DQNAgent(\n",
    "    main_model=main_model,\n",
    "    target_network=target_model,\n",
    "    memory = WeightedReplayBuffer(buffer_size=int(1e5), batch_size=64),\n",
    "    update_every_steps=10, \n",
    "    lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=102, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=102, out_features=77, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=77, out_features=51, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=51, out_features=6, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=6, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_dqn.qnetwork_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   3%|▎         | 16/500 [00:25<12:42,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 16, eps: 0.918\tAverage Score (last 50 episodes): -14915.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   6%|▌         | 30/500 [00:50<15:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 30, eps: 0.856\tAverage Score (last 50 episodes): -14277.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   9%|▉         | 45/500 [01:18<16:09,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 45, eps: 0.794\tAverage Score (last 50 episodes): -13801.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  12%|█▏        | 59/500 [01:44<13:30,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 59, eps: 0.740\tAverage Score (last 50 episodes): -12652.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  14%|█▍        | 72/500 [02:10<14:54,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 72, eps: 0.694\tAverage Score (last 50 episodes): -11420.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  17%|█▋        | 84/500 [02:35<14:31,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 84, eps: 0.653\tAverage Score (last 50 episodes): -10539.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  19%|█▉        | 97/500 [03:02<14:02,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 97, eps: 0.612\tAverage Score (last 50 episodes): -10232.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  22%|██▏       | 110/500 [03:27<12:51,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 110, eps: 0.573\tAverage Score (last 50 episodes): -10809.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  25%|██▍       | 123/500 [03:53<12:07,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 123, eps: 0.537\tAverage Score (last 50 episodes): -10813.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  27%|██▋       | 137/500 [04:20<11:39,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 137, eps: 0.501\tAverage Score (last 50 episodes): -10810.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  30%|███       | 151/500 [04:47<11:05,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 151, eps: 0.467\tAverage Score (last 50 episodes): -10250.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  33%|███▎      | 164/500 [05:13<10:56,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 164, eps: 0.437\tAverage Score (last 50 episodes): -9984.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  35%|███▌      | 176/500 [05:39<11:50,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 176, eps: 0.412\tAverage Score (last 50 episodes): -9983.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  38%|███▊      | 188/500 [06:06<12:09,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 188, eps: 0.388\tAverage Score (last 50 episodes): -9985.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  40%|████      | 200/500 [06:32<10:47,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 200, eps: 0.365\tAverage Score (last 50 episodes): -9996.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  42%|████▏     | 211/500 [07:02<16:06,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 211, eps: 0.346\tAverage Score (last 50 episodes): -9993.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  44%|████▍     | 221/500 [07:27<12:07,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 221, eps: 0.329\tAverage Score (last 50 episodes): -9997.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  46%|████▌     | 230/500 [07:54<11:42,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 230, eps: 0.314\tAverage Score (last 50 episodes): -10002.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  48%|████▊     | 242/500 [08:19<09:08,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 242, eps: 0.296\tAverage Score (last 50 episodes): -9982.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  51%|█████     | 254/500 [08:46<09:26,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 254, eps: 0.279\tAverage Score (last 50 episodes): -9983.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:  53%|█████▎    | 263/500 [09:06<08:49,  2.23s/it]"
     ]
    }
   ],
   "source": [
    "from runner.gym_runner import run as GymRun\n",
    "\n",
    "skiing_scores_dqn = GymRun(\n",
    "        agent_dqn,\n",
    "        env, \n",
    "        render_env=False,\n",
    "        n_episodes=500,\n",
    "    max_t=2000,\n",
    "    feedback_every_secs=25, \n",
    "    keep_last_scores=50,\n",
    "    eps_start=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset() # reset the environment, get the current state\n",
    "total_reward = 0\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = agent_dqn.act(state=state, eps=.09)\n",
    "    next_state, reward, done, info = env.step(action)        # send the action to the environment, get data\n",
    "    total_reward += reward\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:\n",
    "        break\n",
    "print(\"Reward obtained = %.2f\" % (total_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(skiing_scores_dqn)), skiing_scores_dqn)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Skiing scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = dqn(agent, n_episodes=50, feedback_every_secs=15, keep_last_scores=10)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores_dqn)), scores_dqn)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('DQN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gym)",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
